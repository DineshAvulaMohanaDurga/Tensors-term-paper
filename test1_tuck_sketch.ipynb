{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_count_sketch(m,n):\n",
    "    hash=np.random.randint(100,1000)\n",
    "    # hash=rand.randint(100,1000)\n",
    "    # hash*i mod m\n",
    "    omega = np.zeros((m,n))\n",
    "    for i in range(n):\n",
    "        pos = np.mod(hash*i,m)\n",
    "        omega[pos,i]=1\n",
    "    D = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        D[i,i]=np.random.choice([-1,1])\n",
    "    # print(omega)\n",
    "    return omega @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_composite_sketching_mat(s,R,epsilon):\n",
    "    k1 = int(3*(R/epsilon))\n",
    "    k2 = int(3*(R*R + R/epsilon))\n",
    "    T = gen_count_sketch(s,k2)\n",
    "    G= np.random.randn(k2,k1)*(1/np.sqrt(k1))\n",
    "    S = T @ G\n",
    "    # print(np.shape(T))\n",
    "    # print(np.shape(G))\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_RRF(M,R,epsilon):\n",
    "    m =np.shape(M)[1]\n",
    "    S=gen_composite_sketching_mat(m,R,epsilon)\n",
    "    B = M @ S\n",
    "    U,sigma,V = np.linalg.svd(B)\n",
    "    print(np.shape(U))\n",
    "    return U[:,:R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:  1) tensor: Input tensor\n",
    "        2) n: mode along which to matricize the tensor (mode is 0-indexed)\n",
    "Output: matrix: n-mode matricization of the tensor\n",
    "\"\"\"\n",
    "# Take mode = 0 for the first mode, mode = 1 for the second mode, ...., mode = n-1 for nth mode\n",
    "def mode_n_matricization(tensor, n):\n",
    "    mode = n\n",
    "    # Get the size of the original tensor\n",
    "    sz = tensor.size()\n",
    "    # print(f\"tensor size: {sz}\")\n",
    "\n",
    "    # Permute the dimensions of the tensor to bring the chosen mode to the front\n",
    "    # This will make it easy to reshape the tensor into a matrix along the chosen mode\n",
    "    permuted_dimensions = list(range(len(sz)))\n",
    "#     print('Before Permuation, dimensions: ', permuted_dimensions)\n",
    "    permuted_dimensions.remove(mode)\n",
    "    permuted_dimensions.insert(0, mode)\n",
    "#     print('After Permuation, dimensions: ', permuted_dimensions)\n",
    "    permuted_tensor = tensor.permute(*permuted_dimensions)\n",
    "#     print(f\"permuted tensor size: {permuted_tensor.size()}\")\n",
    "\n",
    "    # Reshape the permuted tensor into a matrix along the chosen mode\n",
    "    matrix = permuted_tensor.reshape(sz[mode], -1)\n",
    "#     print(f\"matrix size: {matrix.size()}\")\n",
    "\n",
    "#     print(f\"n-mode matricization along mode {mode}:\")\n",
    "#     print(matrix)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSVD_LRLS(Z, Y, R):\n",
    "\n",
    "    m,s = Y.shape\n",
    "\n",
    "    S = np.random.randn(s, 5*R)\n",
    "    B = np.linalg.inv((Z.T @ Z))\n",
    "    C= B @ Z.T @  Y @ S\n",
    "\n",
    "    Q, r = np.linalg.qr(C)\n",
    "\n",
    "    D = Q.T @ B @ Z.T @ Y\n",
    "\n",
    "    U, sig , V = np.linalg.svd(D)\n",
    "\n",
    "    sig = np.diag(sig)\n",
    "\n",
    "    print(U.shape)\n",
    "    print(sig.shape)\n",
    "    print(V.shape)\n",
    "\n",
    "    return Q @ U[:, :R] @ sig[:R, :R] , V[:, :R]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "(8, 5)\n",
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "# Testing RSVD-LRLS\n",
    "\n",
    "m = 10\n",
    "s = 5 \n",
    "r = 8\n",
    "R = 5\n",
    "\n",
    "Z = np.random.randn(m, r)\n",
    "Y = np.random.randn(m, s)\n",
    "\n",
    "C , A = RSVD_LRLS(Z,Y,R)\n",
    "\n",
    "print(C.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tensor_sketch(m,sn):\n",
    "    hash1 = np.random.randint(100,1000)\n",
    "    hash2 = np.random.randint(100,1000)\n",
    "    hash3 = np.random.randint(100,1000)\n",
    "    hash4 = np.random.randint(100,1000)\n",
    "\n",
    "    omega = np.zeros((m,sn))\n",
    "    for i in range(sn):\n",
    "        pos = np.mod(hash1 * (i **3) + hash2 * (i ** 2) + hash3 * (i) ,m)    # 3 wise independent hashes\n",
    "        omega[pos,i]=1\n",
    "    D = np.zeros((sn,sn))\n",
    "    for i in range(sn):\n",
    "        hash = np.mod(hash4 * (i **4) + hash1 * (i **3) + hash2 * (i ** 2) + hash3 * (i) ,2)   # 4 wise independent hashes\n",
    "        D[i,i] = -1 if hash == 0 else 1\n",
    "    return omega @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equation: abcde,za,yb,xc,wd,ve->zyxwv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Input: T: Input tensor\n",
    "Output: equation: equation string for torch.einsum()\n",
    "\"\"\"\n",
    "def einsum_equation(T):\n",
    "    equation = ''\n",
    "    for i in range(len(T.size())): \n",
    "        equation += f'{chr(97 + i)}'\n",
    "    for i in range(len(T.size())):\n",
    "        equation += f',{chr(122 -i)}{chr(97 + i)}'\n",
    "    equation += '->'\n",
    "    for i in range(len(T.size())):\n",
    "        equation += f'{chr(122 - i)}'\n",
    "    # print(f\"equation: {equation}\")\n",
    "    return equation\n",
    "\n",
    "T = torch.randn((2,3,4,5,6))\n",
    "eqn = einsum_equation(T)\n",
    "print(f\"equation: {eqn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sketch_Tucker_ALS(T, ranks, Imax=100, epsilon=0.1):\n",
    "\n",
    "    N = len(ranks)\n",
    "    C = np.zeros(ranks)\n",
    "\n",
    "    A = [None]\n",
    "    for n in range(1, N):\n",
    "        T_tensor = torch.Tensor(T) \n",
    "        A.append(Init_RRF(mode_n_matricization(T_tensor.numpy(), n), ranks[n], epsilon))\n",
    "\n",
    "    fitness_arr = []\n",
    "\n",
    "    \n",
    "    for i in range(Imax):\n",
    "        for n in range(N):\n",
    "            # Build sketching S\n",
    "            shape = T.shape\n",
    "            shp = 1\n",
    "            for s in range(len(shape)):\n",
    "                if s != n :\n",
    "                    shp = shp * shape[s]\n",
    "\n",
    "            Sn = gen_tensor_sketch(m, shp)\n",
    "            T_tensor = torch.Tensor(T)\n",
    "            Y = Sn @ (mode_n_matricization(T_tensor, n).numpy()).T\n",
    "\n",
    "            if n == 0:\n",
    "                A_kronecker = A[1]\n",
    "                for j in range(2, N):\n",
    "                    if j != n:\n",
    "                        A_kronecker = np.kron(A_kronecker, A[j])\n",
    "            else:\n",
    "                A_kronecker = A[0]\n",
    "                for j in range(1, N):\n",
    "                    if j != n:\n",
    "                        A_kronecker = np.kron(A_kronecker, A[j])\n",
    "            \n",
    "            print(f'n = {n} and Sn = {Sn.shape}')\n",
    "            print(f'n = {n} and Akron = {A_kronecker.shape}')\n",
    "\n",
    "            Z = Sn @ A_kronecker\n",
    "            C_matrix, A[n] = RSVD_LRLS(Z, Y, ranks[n]) \n",
    "\n",
    "            C = mode_n_matricization( torch.Tensor(C), n).numpy()\n",
    "            C = C_matrix.T\n",
    "            C = C.reshape(ranks)\n",
    "\n",
    "            T_tensor = torch.tensor(T)\n",
    "            eqn = einsum_equation(T_tensor)\n",
    "            A_T = [A[i].T for i in range(1,len(A))]\n",
    "            print(type(T_tensor), type(A_T))\n",
    "            C = torch.einsum(eqn, T_tensor, *A_T) \n",
    "            fitness=1 - torch.norm(T_tensor - torch.einsum(eqn,C,*A))/torch.norm(T_tensor)\n",
    "            fitness_arr.append(fitness)\n",
    "            C = C.numpy()\n",
    "\n",
    "        return (C, A, fitness_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing Sketch Tucker\n",
    "#  Error prone code since matrices are random\n",
    "# tensor = np.random.randn(10,10,10)\n",
    "# ranks = (5,5,5)\n",
    "# imax = 5\n",
    "# eps = 0.1\n",
    "\n",
    "# Sketch_Tucker_ALS(tensor, ranks, imax, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A: Array of matrices whose Khatri-Rao product is to be calculated\n",
    "Output: B: Khatri-Rao product of the matrices in A\n",
    "\"\"\"\n",
    "def khatri_rao_product(A):\n",
    "    N = len(A)\n",
    "    num_cols = A[0].shape[1]\n",
    "    x_dim = 1\n",
    "    for i in range(N):\n",
    "        x_dim *= A[i].shape[0]\n",
    "    B = torch.empty((x_dim, num_cols))\n",
    "\n",
    "    for i in range(num_cols):\n",
    "        for j in range(N):\n",
    "            if j == 0:\n",
    "                kron_prod = A[j][:, i]\n",
    "            else:\n",
    "                kron_prod = torch.kron(kron_prod, A[j][:, i])\n",
    "        B[:, i] = kron_prod\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:  1) T: Tensor of size (s_1, s_2, ..., s_N)\n",
    "        2) R: CP-Rank of the Tensor\n",
    "Output: 1) A: Array of N factor matrices of size (s_i, R)\n",
    "\"\"\"\n",
    "def CP_ALS(T, R, max_iter=100, tol=1e-5):\n",
    "    N = len(T.shape)\n",
    "    sz = T.shape\n",
    "    A = []\n",
    "    for n in range(N):\n",
    "        An = torch.rand(sz[n], R)\n",
    "        A.append(An)\n",
    "\n",
    "    # Iterate until convergence\n",
    "    for iter in range(max_iter):\n",
    "        prev_factors = A.copy()\n",
    "\n",
    "        for n in range(N):\n",
    "            # Calculate the Khatri-Rao product of all the factor matrices except the n-th factor matrix\n",
    "            B = khatri_rao_product(A[:n] + A[n+1:])\n",
    "            B_pinv = torch.pinverse(B)\n",
    "\n",
    "            # Calculate the n-mode matricization of the tensor\n",
    "            Tn = mode_n_matricization(T, n)\n",
    "            \n",
    "            # Calculate the n-th factor matrix\n",
    "            AnT = torch.mm(B_pinv, Tn.T)\n",
    "            A[n] = AnT.T\n",
    "\n",
    "        # Check for convergence\n",
    "        if all(torch.norm(A[i] - prev_factors[i]) < tol for i in range(T.dim())):\n",
    "            print(f\"Converged at iteration {iter}\")\n",
    "            break\n",
    "\n",
    "    return A  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CP_Sketch_Tucker(T, R, Imax, epsilon):\n",
    "\n",
    "    N = T.ndim\n",
    "    ranks = tuple([R] * N)\n",
    "    C, B = Sketch_Tucker_ALS(T, ranks, Imax, epsilon)\n",
    "    A = CP_ALS(C, R)\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(A)):\n",
    "        result.append(B[i] @ A[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[1;32m      4\u001b[0m ranks \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m C, A, fitness_arr \u001b[39m=\u001b[39m Sketch_Tucker_ALS(tensor\u001b[39m.\u001b[39;49mnumpy(), ranks)\n\u001b[1;32m      6\u001b[0m itera\u001b[39m=\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(fitness_arr))]\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mplot(itera, fitness_arr)\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mSketch_Tucker_ALS\u001b[0;34m(T, ranks, Imax, epsilon)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, N):\n\u001b[1;32m      8\u001b[0m     T_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(T) \n\u001b[0;32m----> 9\u001b[0m     A\u001b[39m.\u001b[39mappend(Init_RRF(mode_n_matricization(T_tensor\u001b[39m.\u001b[39;49mnumpy(), n), ranks[n], epsilon))\n\u001b[1;32m     11\u001b[0m fitness_arr \u001b[39m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Imax):\n",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m, in \u001b[0;36mmode_n_matricization\u001b[0;34m(tensor, n)\u001b[0m\n\u001b[1;32m      8\u001b[0m mode \u001b[39m=\u001b[39m n\n\u001b[1;32m      9\u001b[0m \u001b[39m# Get the size of the original tensor\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sz \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m     11\u001b[0m \u001b[39m# print(f\"tensor size: {sz}\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[39m# Permute the dimensions of the tensor to bring the chosen mode to the front\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# This will make it easy to reshape the tensor into a matrix along the chosen mode\u001b[39;00m\n\u001b[1;32m     15\u001b[0m permuted_dimensions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sz)))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "#Now plot\n",
    "\n",
    "tensor = torch.randn((2,3,4,5))\n",
    "ranks = (1,2,2,3)\n",
    "C, A, fitness_arr = Sketch_Tucker_ALS(tensor.numpy(), ranks)\n",
    "itera=[i for i in range(len(fitness_arr))]\n",
    "plt.plot(itera, fitness_arr)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Fitness')\n",
    "plt.title('Fitness vs. Iteration')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
